{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Docker\n",
    "==\n",
    "\n",
    "Docker es una herramienta que ya lleva varios años tomando popularidad en los ambientes de desarrollo de software. Este popularidad se va extendiendo a otras areas más allá del desarrollo de software. ¿Por qué?\n",
    "\n",
    "1. Motivación al uso de docker\n",
    "2. Dockerfile\n",
    "3. Dockerfile en la ciencia de datos\n",
    "4. Uso de Spark Local\n",
    "    1. integración con **pandas**, integración con **s3**\n",
    "    2. machine learning con spark: **kmeans**\n",
    "5. Generación de Librerías y Reutilización de Código y proyectos\n",
    "6. Subir nuestra Librería a github\n",
    "    1. Crear una cuenta github\n",
    "    2. Subir nuestro código y notebooks a github público\n",
    "7. Tarea aplicada: Librería de regresión lineal con sklearn + pandas + github\n",
    "8. Tarea aplicada: Librería de k-means con sklearn + pandas + github\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Bibliografía\n",
    "\n",
    "1. [Docker for Data Science: Building Scalable and Extensible Data Infrastructure Around the Jupyter Notebook Server](https://libgen.is/book/index.php?md5=7521B3D87DF447AB03F259271A5C2149)\n",
    "2. [Learning PySpark](https://libgen.is/book/index.php?md5=8A78A93D5EB86F511A74910DC09E3DD6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Docker + Pyspark\n",
    "==\n",
    "\n",
    "Si usted no sabe pyspark no sabe big data, o por lo menos, big data actual.\n",
    "\n",
    "![](https://echanclarityinsights.github.io/images/2018-12-23/spark-meme.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.context import SQLContext\n",
    "from pyspark.sql.session import SparkSession "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.hadoop:hadoop-aws:2.7.3 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master(\"local\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tareas guiadas:\n",
    "0. Construir unas credenciales para el cliente en Amazon Web Services\n",
    "1. Leer Iris con pyspark\n",
    "2. Leer Iris con pyspark y guardarlo como parquet\n",
    "3. Guardarlo en un bucket s3 particionado por tipo de flor\n",
    "4. Encontrar mediante sql las dos flores del mismo tipo más \"diferentes\" mediante distancia Euclidiana"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ronald Fisher** tiene como idea determinar cuales pares de flores del tipo iris son los más distintos posibles pero que son del mismo tipo, diseñe un programa en pyspark que de respuesta ¿cuáles son los pares más distintos posibles siendo que son del mismo tipo de flor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_c0: string, _c1: string, _c2: string, _c3: string, _c4: string]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.csv(\"iris.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+-------+\n",
      "|sepal.length|sepal.width|petal.length|petal.width|variety|\n",
      "+------------+-----------+------------+-----------+-------+\n",
      "|         5.1|        3.5|         1.4|         .2| Setosa|\n",
      "|         4.9|          3|         1.4|         .2| Setosa|\n",
      "|         4.7|        3.2|         1.3|         .2| Setosa|\n",
      "|         4.6|        3.1|         1.5|         .2| Setosa|\n",
      "|           5|        3.6|         1.4|         .2| Setosa|\n",
      "|         5.4|        3.9|         1.7|         .4| Setosa|\n",
      "|         4.6|        3.4|         1.4|         .3| Setosa|\n",
      "|           5|        3.4|         1.5|         .2| Setosa|\n",
      "|         4.4|        2.9|         1.4|         .2| Setosa|\n",
      "|         4.9|        3.1|         1.5|         .1| Setosa|\n",
      "|         5.4|        3.7|         1.5|         .2| Setosa|\n",
      "|         4.8|        3.4|         1.6|         .2| Setosa|\n",
      "|         4.8|          3|         1.4|         .1| Setosa|\n",
      "|         4.3|          3|         1.1|         .1| Setosa|\n",
      "|         5.8|          4|         1.2|         .2| Setosa|\n",
      "|         5.7|        4.4|         1.5|         .4| Setosa|\n",
      "|         5.4|        3.9|         1.3|         .4| Setosa|\n",
      "|         5.1|        3.5|         1.4|         .3| Setosa|\n",
      "|         5.7|        3.8|         1.7|         .3| Setosa|\n",
      "|         5.1|        3.8|         1.5|         .3| Setosa|\n",
      "+------------+-----------+------------+-----------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iris = (spark\n",
    " .read\n",
    " .options(header=True)\n",
    " .csv(\"iris.csv\")\n",
    ")\n",
    "\n",
    "iris.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- sepal.length: string (nullable = true)\n",
      " |-- sepal.width: string (nullable = true)\n",
      " |-- petal.length: string (nullable = true)\n",
      " |-- petal.width: string (nullable = true)\n",
      " |-- variety: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iris.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = spark.read.options(header=True, inferSchema=True).csv(\"iris.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para poder identificar cada una de las realizaciones observadas incorporaremos un identidicador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "iris = iris.withColumn(\"id\", monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+-------+---+\n",
      "|sepal.length|sepal.width|petal.length|petal.width|variety| id|\n",
      "+------------+-----------+------------+-----------+-------+---+\n",
      "|         5.1|        3.5|         1.4|        0.2| Setosa|  0|\n",
      "|         4.9|        3.0|         1.4|        0.2| Setosa|  1|\n",
      "|         4.7|        3.2|         1.3|        0.2| Setosa|  2|\n",
      "|         4.6|        3.1|         1.5|        0.2| Setosa|  3|\n",
      "|         5.0|        3.6|         1.4|        0.2| Setosa|  4|\n",
      "|         5.4|        3.9|         1.7|        0.4| Setosa|  5|\n",
      "|         4.6|        3.4|         1.4|        0.3| Setosa|  6|\n",
      "|         5.0|        3.4|         1.5|        0.2| Setosa|  7|\n",
      "|         4.4|        2.9|         1.4|        0.2| Setosa|  8|\n",
      "|         4.9|        3.1|         1.5|        0.1| Setosa|  9|\n",
      "|         5.4|        3.7|         1.5|        0.2| Setosa| 10|\n",
      "|         4.8|        3.4|         1.6|        0.2| Setosa| 11|\n",
      "|         4.8|        3.0|         1.4|        0.1| Setosa| 12|\n",
      "|         4.3|        3.0|         1.1|        0.1| Setosa| 13|\n",
      "|         5.8|        4.0|         1.2|        0.2| Setosa| 14|\n",
      "|         5.7|        4.4|         1.5|        0.4| Setosa| 15|\n",
      "|         5.4|        3.9|         1.3|        0.4| Setosa| 16|\n",
      "|         5.1|        3.5|         1.4|        0.3| Setosa| 17|\n",
      "|         5.7|        3.8|         1.7|        0.3| Setosa| 18|\n",
      "|         5.1|        3.8|         1.5|        0.3| Setosa| 19|\n",
      "+------------+-----------+------------+-----------+-------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iris.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tipo Parquet\n",
    "\n",
    "Incorporaremos un nuevo de tipo de dato al ya conocido csv, el tipo parquet:\n",
    "\n",
    "El formato Parquet es un formato open-source de almacenamiento en columnas para Hadoop.\n",
    "\n",
    "Fue creado para poder disponer de un formato libre de compresión y codificación eficiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.write.mode(\"overwrite\").parquet(\"./iris\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris2 = iris\n",
    "\n",
    "iris_join = iris.join(iris2, iris.variety == iris2.variety)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sepal.length',\n",
       " 'sepal.width',\n",
       " 'petal.length',\n",
       " 'petal.width',\n",
       " 'variety',\n",
       " 'id',\n",
       " 'sepal.length',\n",
       " 'sepal.width',\n",
       " 'petal.length',\n",
       " 'petal.width',\n",
       " 'variety',\n",
       " 'id']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_join.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_join = iris_join.toDF(*[\"sepal_length\", \n",
    "                     \"sepal_width\", \n",
    "                     \"petal_length\", \n",
    "                     \"petal_width\", \n",
    "                     \"variety\", \n",
    "                     \"id\",\n",
    "                     \"sepal_length2\", \n",
    "                     \"sepal_width2\", \n",
    "                     \"petal_length2\", \n",
    "                     \"petal_width2\", \n",
    "                     \"variety2\",\n",
    "                     \"id2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_join = iris_join.withColumn(\"distancia\", (iris_join.sepal_length - iris_join.sepal_length2)**2  + (iris_join.sepal_width - iris_join.sepal_width2)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import max\n",
    "\n",
    "resume = iris_join.groupBy(\"variety\").agg(max(\"distancia\").alias(\"distancia\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+---+-----------------+\n",
      "|  variety2| id|id2|        distancia|\n",
      "+----------+---+---+-----------------+\n",
      "|    Setosa| 15| 41|5.850000000000002|\n",
      "|    Setosa| 41| 15|5.850000000000002|\n",
      "|Versicolor| 50| 60|             5.44|\n",
      "|Versicolor| 60| 50|             5.44|\n",
      "| Virginica|106|131|            10.69|\n",
      "| Virginica|131|106|            10.69|\n",
      "+----------+---+---+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(iris_join\n",
    "     .join(resume,iris_join.distancia ==  resume.distancia, \"inner\")\n",
    "     .select(iris_join.variety2, iris_join.id,iris_join.id2,iris_join.distancia)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![kmedias](https://www.datascience.com/hs-fs/hubfs/k-means-alternative-2.png?width=875&name=k-means-alternative-2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|   variety|\n",
      "+----------+\n",
      "| Virginica|\n",
      "|    Setosa|\n",
      "|Versicolor|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iris.select(iris.variety).distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris3 = iris.toDF(*[\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\", \"variety\", \"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+-------+---+-----------------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|variety| id|         features|\n",
      "+------------+-----------+------------+-----------+-------+---+-----------------+\n",
      "|         5.1|        3.5|         1.4|        0.2| Setosa|  0|[5.1,3.5,1.4,0.2]|\n",
      "|         4.9|        3.0|         1.4|        0.2| Setosa|  1|[4.9,3.0,1.4,0.2]|\n",
      "|         4.7|        3.2|         1.3|        0.2| Setosa|  2|[4.7,3.2,1.3,0.2]|\n",
      "|         4.6|        3.1|         1.5|        0.2| Setosa|  3|[4.6,3.1,1.5,0.2]|\n",
      "|         5.0|        3.6|         1.4|        0.2| Setosa|  4|[5.0,3.6,1.4,0.2]|\n",
      "|         5.4|        3.9|         1.7|        0.4| Setosa|  5|[5.4,3.9,1.7,0.4]|\n",
      "|         4.6|        3.4|         1.4|        0.3| Setosa|  6|[4.6,3.4,1.4,0.3]|\n",
      "|         5.0|        3.4|         1.5|        0.2| Setosa|  7|[5.0,3.4,1.5,0.2]|\n",
      "|         4.4|        2.9|         1.4|        0.2| Setosa|  8|[4.4,2.9,1.4,0.2]|\n",
      "|         4.9|        3.1|         1.5|        0.1| Setosa|  9|[4.9,3.1,1.5,0.1]|\n",
      "|         5.4|        3.7|         1.5|        0.2| Setosa| 10|[5.4,3.7,1.5,0.2]|\n",
      "|         4.8|        3.4|         1.6|        0.2| Setosa| 11|[4.8,3.4,1.6,0.2]|\n",
      "|         4.8|        3.0|         1.4|        0.1| Setosa| 12|[4.8,3.0,1.4,0.1]|\n",
      "|         4.3|        3.0|         1.1|        0.1| Setosa| 13|[4.3,3.0,1.1,0.1]|\n",
      "|         5.8|        4.0|         1.2|        0.2| Setosa| 14|[5.8,4.0,1.2,0.2]|\n",
      "|         5.7|        4.4|         1.5|        0.4| Setosa| 15|[5.7,4.4,1.5,0.4]|\n",
      "|         5.4|        3.9|         1.3|        0.4| Setosa| 16|[5.4,3.9,1.3,0.4]|\n",
      "|         5.1|        3.5|         1.4|        0.3| Setosa| 17|[5.1,3.5,1.4,0.3]|\n",
      "|         5.7|        3.8|         1.7|        0.3| Setosa| 18|[5.7,3.8,1.7,0.3]|\n",
      "|         5.1|        3.8|         1.5|        0.3| Setosa| 19|[5.1,3.8,1.5,0.3]|\n",
      "+------------+-----------+------------+-----------+-------+---+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "vecAssembler = VectorAssembler(inputCols=[\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"], \n",
    "                               outputCol=\"features\")\n",
    "new_df = vecAssembler.transform(iris3)\n",
    "new_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "kmeans = KMeans(k=3, seed=1)  # 2 clusters here\n",
    "model = kmeans.fit(new_df.select('features'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+-------+---+-----------------+----------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|variety| id|         features|prediction|\n",
      "+------------+-----------+------------+-----------+-------+---+-----------------+----------+\n",
      "|         5.1|        3.5|         1.4|        0.2| Setosa|  0|[5.1,3.5,1.4,0.2]|         2|\n",
      "|         4.9|        3.0|         1.4|        0.2| Setosa|  1|[4.9,3.0,1.4,0.2]|         2|\n",
      "|         4.7|        3.2|         1.3|        0.2| Setosa|  2|[4.7,3.2,1.3,0.2]|         2|\n",
      "|         4.6|        3.1|         1.5|        0.2| Setosa|  3|[4.6,3.1,1.5,0.2]|         2|\n",
      "|         5.0|        3.6|         1.4|        0.2| Setosa|  4|[5.0,3.6,1.4,0.2]|         2|\n",
      "|         5.4|        3.9|         1.7|        0.4| Setosa|  5|[5.4,3.9,1.7,0.4]|         2|\n",
      "|         4.6|        3.4|         1.4|        0.3| Setosa|  6|[4.6,3.4,1.4,0.3]|         2|\n",
      "|         5.0|        3.4|         1.5|        0.2| Setosa|  7|[5.0,3.4,1.5,0.2]|         2|\n",
      "|         4.4|        2.9|         1.4|        0.2| Setosa|  8|[4.4,2.9,1.4,0.2]|         2|\n",
      "|         4.9|        3.1|         1.5|        0.1| Setosa|  9|[4.9,3.1,1.5,0.1]|         2|\n",
      "|         5.4|        3.7|         1.5|        0.2| Setosa| 10|[5.4,3.7,1.5,0.2]|         2|\n",
      "|         4.8|        3.4|         1.6|        0.2| Setosa| 11|[4.8,3.4,1.6,0.2]|         2|\n",
      "|         4.8|        3.0|         1.4|        0.1| Setosa| 12|[4.8,3.0,1.4,0.1]|         2|\n",
      "|         4.3|        3.0|         1.1|        0.1| Setosa| 13|[4.3,3.0,1.1,0.1]|         2|\n",
      "|         5.8|        4.0|         1.2|        0.2| Setosa| 14|[5.8,4.0,1.2,0.2]|         2|\n",
      "|         5.7|        4.4|         1.5|        0.4| Setosa| 15|[5.7,4.4,1.5,0.4]|         2|\n",
      "|         5.4|        3.9|         1.3|        0.4| Setosa| 16|[5.4,3.9,1.3,0.4]|         2|\n",
      "|         5.1|        3.5|         1.4|        0.3| Setosa| 17|[5.1,3.5,1.4,0.3]|         2|\n",
      "|         5.7|        3.8|         1.7|        0.3| Setosa| 18|[5.7,3.8,1.7,0.3]|         2|\n",
      "|         5.1|        3.8|         1.5|        0.3| Setosa| 19|[5.1,3.8,1.5,0.3]|         2|\n",
      "+------------+-----------+------------+-----------+-------+---+-----------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transformed = model.transform(new_df)\n",
    "transformed.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-----+\n",
      "|   variety|prediction|count|\n",
      "+----------+----------+-----+\n",
      "|Versicolor|         0|   47|\n",
      "|    Setosa|         2|   50|\n",
      "| Virginica|         1|   36|\n",
      "|Versicolor|         1|    3|\n",
      "| Virginica|         0|   14|\n",
      "+----------+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(transformed\n",
    " .select([transformed.variety, transformed.prediction])\n",
    " .groupby([transformed.variety, transformed.prediction])\n",
    " .count()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nspark.conf.set(\"fs.s3n.awsAccessKeyId\", aws_access_key_id)\\nspark.conf.set(\"fs.s3n.awsSecretAccessKey\", aws_secret_access_key)\\nspark.conf.set(\"fs.s3.awsAccessKeyId\", aws_access_key_id)\\nspark.conf.set(\"fs.s3.awsSecretAccessKey\", aws_secret_access_key)\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "spark.conf.set(\"fs.s3n.awsAccessKeyId\", aws_access_key_id)\n",
    "spark.conf.set(\"fs.s3n.awsSecretAccessKey\", aws_secret_access_key)\n",
    "spark.conf.set(\"fs.s3.awsAccessKeyId\", aws_access_key_id)\n",
    "spark.conf.set(\"fs.s3.awsSecretAccessKey\", aws_secret_access_key)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
